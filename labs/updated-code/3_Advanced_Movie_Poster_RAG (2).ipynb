{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QptZcwMEBim2"
      },
      "source": [
        "# Advanced Agentic AI: Multi-Modal RAG for Movie Posters\n",
        "\n",
        "This notebook builds a production-grade, multi-modal RAG agent. This advanced version includes:\n",
        "\n",
        "1.  **Conversational Memory**: The RAG agent remembers past questions for follow-up queries.\n",
        "2.  **Advanced Retrieval**: It extracts entities from queries to perform metadata filtering for more accurate results.\n",
        "3.  **Self-Correction Loop**: The agent validates its retrieval results and can ask for clarification if needed.\n",
        "4.  **Enhanced Explainability**: The final output shows the exact context used by the LLM to generate an answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es7WBTftBim7"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NoYZFzcrBim8"
      },
      "outputs": [],
      "source": [
        "#!pip install -q langgraph openai pinecone python-dotenv pillow tqdm langchain langchain_openai langgraph-checkpoint-sqlite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dXEElTOBim9"
      },
      "source": [
        "## 2. Imports and Configuration Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "se9LvuH5Bim-"
      },
      "outputs": [],
      "source": [
        "import os  # For interacting with the operating system, like managing file paths.\n",
        "import base64  # For encoding image files into a text string to send to vision models.\n",
        "import configparser  # For reading settings and prompts from the .ini configuration files.\n",
        "import sqlite3  # For creating the connection to the local SQLite database for memory.\n",
        "import uuid  # For generating unique IDs for each conversational thread.\n",
        "from dotenv import load_dotenv  # For loading secret API keys from the .env file.\n",
        "from openai import OpenAI  # The main client for making API calls to OpenAI models.\n",
        "from pinecone import Pinecone as PineconeClient, ServerlessSpec  # The client for managing the Pinecone vector database.\n",
        "from IPython.display import Markdown, display, Image as IPImage  # For displaying formatted text and images in the notebook output.\n",
        "from typing import TypedDict, List, Dict, Annotated  # For creating typed data structures for our agent's state.\n",
        "from langgraph.graph import StateGraph, END, START  # Core components for building the agent's workflow graph.\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver  # The specific class for saving the agent's state to a SQLite file.\n",
        "from langgraph.graph.message import add_messages  # A helper to correctly manage conversational history in the state.\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage  # Standard classes for representing messages in the chat history.\n",
        "from PIL import Image  # The Python Imaging Library, used for opening and processing image files.\n",
        "from tqdm import tqdm  # A library for creating progress bars to monitor long loops.\n",
        "import warnings  # For managing warning messages displayed in the notebook.\n",
        "\n",
        "# Suppress common, non-critical warnings to keep the output clean.\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "riulkzJ0Bim_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b57b059-0645-4710-f134-6ffcaa137f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys loaded.\n",
            "Configuration loaded.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    load_dotenv(\"dev.env\")\n",
        "    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "    PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "    if not OPENAI_API_KEY or not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API keys not found.\")\n",
        "    print(\"API keys loaded.\")\n",
        "\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read('3-config.ini')\n",
        "\n",
        "    IMAGE_DIRECTORY = config.get('Paths', 'image_directory')\n",
        "    OCR_MODEL = config.get('Models', 'ocr_model')\n",
        "    DESCRIPTION_MODEL = config.get('Models', 'description_model')\n",
        "    EMBEDDING_MODEL = config.get('Models', 'embedding_model')\n",
        "    RAG_MODEL = config.get('Models', 'rag_model')\n",
        "    FAST_REASONING_MODEL = config.get('Models', 'fast_reasoning_model')\n",
        "    PINECONE_INDEX_NAME = config.get('Pinecone', 'index_name')\n",
        "    IMAGE_LIMIT = config.getint('Parameters', 'image_limit', fallback=0)\n",
        "    PINECONE_BATCH_SIZE = config.getint('Parameters', 'pinecone_batch_size', fallback=100)\n",
        "    TOP_K_RETRIEVAL = config.getint('Parameters', 'top_k_retrieval', fallback=3)\n",
        "    OCR_PROMPT = config.get('Prompts', 'ocr_prompt')\n",
        "    DESCRIPTION_PROMPT = config.get('Prompts', 'description_prompt')\n",
        "    RAG_PROMPT = config.get('Prompts', 'rag_prompt')\n",
        "    print(\"Configuration loaded.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during setup: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gESb9kVBim_"
      },
      "source": [
        "# Phase 1: The Indexing Pipeline (Unchanged)\n",
        "\n",
        "This phase remains the same. It is a one-time process to build our knowledge base in Pinecone. If you have already run this successfully, you can skip these cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0rs7CvNlBim_"
      },
      "outputs": [],
      "source": [
        "# ======================================================================================\n",
        "# This class defines the 'state' for our Indexing Pipeline.\n",
        "# Think of it as a structured \"job folder\" that gets passed between each step (node)\n",
        "# in our graph. Using a TypedDict ensures that the data at each step has a\n",
        "# consistent and predictable structure, which helps prevent bugs.\n",
        "#\n",
        "# Fields:\n",
        "#   - image_paths: A list that will hold the file paths of all images found\n",
        "#                  in the source directory.\n",
        "#   - processed_data: A list that will store the results of the analysis. Each item\n",
        "#                     will be a dictionary containing the extracted text and visual\n",
        "#                     description for a single poster.\n",
        "#   - error_message: A string that acts as a status flag. If any node encounters\n",
        "#                    a problem, it will record the error message here, allowing\n",
        "#                    the graph to handle failures gracefully.\n",
        "# ======================================================================================\n",
        "\n",
        "class IndexingState(TypedDict):\n",
        "    image_paths: List[str]\n",
        "    # Add new fields to store paths to text and video files\n",
        "    text_paths: List[str]\n",
        "    video_paths: List[str]\n",
        "    # The processed data list will now store all data types\n",
        "    processed_data: List[Dict]\n",
        "    error_message: str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_base64(image_path):\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            img = img.convert(\"RGB\")\n",
        "            img.thumbnail((1024, 1024))\n",
        "            import io\n",
        "            buffer = io.BytesIO()\n",
        "            img.save(buffer, format=\"JPEG\")\n",
        "            return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "    except Exception as e:\n",
        "        return None"
      ],
      "metadata": {
        "id": "Kc0HlOJ1Eq3Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "DKlTUIVVEqt0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_paths_node(state: IndexingState):\n",
        "    print(\"--- NODE: Loading Image Paths ---\")\n",
        "    try:\n",
        "        paths = [os.path.join(IMAGE_DIRECTORY, f) for f in os.listdir(IMAGE_DIRECTORY) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        if IMAGE_LIMIT > 0:\n",
        "            paths = paths[:IMAGE_LIMIT]\n",
        "        print(f\"Found {len(paths)} images to process.\")\n",
        "        return {\"image_paths\": paths}\n",
        "    except Exception as e:\n",
        "        return {\"error_message\": f\"Failed to load images: {e}\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "Pm87ZOqUEqYB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! #rm -rf posters/"
      ],
      "metadata": {
        "id": "NtxyO89cbk9S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_and_video_paths_node(state: IndexingState):\n",
        "    print(\"--- NODE: Loading Text and Video Paths ---\")\n",
        "    try:\n",
        "        # Load text file paths\n",
        "        text_paths = [os.path.join(IMAGE_DIRECTORY, f) for f in os.listdir(IMAGE_DIRECTORY) if f.lower().endswith(('.txt'))]\n",
        "\n",
        "        # Load video file paths (conceptual for this example)\n",
        "        video_paths = [os.path.join(IMAGE_DIRECTORY, f) for f in os.listdir(IMAGE_DIRECTORY) if f.lower().endswith(('.mp4', '.avi'))]\n",
        "\n",
        "        print(f\"Found {len(text_paths)} text files and {len(video_paths)} video files.\")\n",
        "        return {\"text_paths\": text_paths, \"video_paths\": video_paths}\n",
        "    except Exception as e:\n",
        "        return {\"error_message\": f\"Failed to load text/video paths: {e}\"}"
      ],
      "metadata": {
        "id": "CVb46YabVGMw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_text_to_file(text_content, file_path):\n",
        "    \"\"\"Saves the given text content to a specified file path.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(text_content)\n",
        "        print(f\"Saved OCR text to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving file {file_path}: {e}\")"
      ],
      "metadata": {
        "id": "oN2-LAGSUJbS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_images_node(state: IndexingState):\n",
        "    # Print a heading to indicate this processing stage\n",
        "    print(\"--- NODE: Analyzing Images ---\")\n",
        "    pc = PineconeClient(api_key=PINECONE_API_KEY)\n",
        "\n",
        "    # Check if the index already exists\n",
        "    if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
        "        # If the index does not exist, create it\n",
        "        pc.create_index(\n",
        "            name=PINECONE_INDEX_NAME,\n",
        "            dimension=1536,  # dimensionality of the embedding vector\n",
        "            metric=\"cosine\", # similarity metric\n",
        "            spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\") # serverless index config\n",
        "        )\n",
        "\n",
        "    index = pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "    # List to store the final processed results\n",
        "    processed_data = []\n",
        "\n",
        "    # Loop through all image paths using tqdm for progress bar display\n",
        "    ktr=0\n",
        "    for path in tqdm(state['image_paths'], desc=\"Analyzing Posters\"):\n",
        "        if ktr>10:\n",
        "          break\n",
        "        ktr=ktr+1\n",
        "        vector_id = os.path.basename(path)\n",
        "        # Query Pinecone to check if vector exists\n",
        "        result = index.fetch(ids=[vector_id])\n",
        "\n",
        "        if result.vectors:\n",
        "          print(f\"Skipping analysis for {vector_id}, vector already exists.\")\n",
        "          continue\n",
        "\n",
        "        # Convert the image at the given path to a base64-encoded string\n",
        "        base64_image = image_to_base64(path)\n",
        "\n",
        "        # Skip the image if conversion failed or returned empty\n",
        "        if not base64_image:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Step 1: Perform OCR (Optical Character Recognition) using a language model API\n",
        "            ocr_response = client.chat.completions.create(\n",
        "                model=OCR_MODEL,  # model used for OCR\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": OCR_PROMPT},  # instruction prompt\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}  # image data\n",
        "                    ]}\n",
        "                ]\n",
        "            )\n",
        "            # Extract the OCR result text from the response\n",
        "            ocr_text = ocr_response.choices[0].message.content\n",
        "            if not \"I'm sorry\" in ocr_text:\n",
        "              base_filename = os.path.splitext(os.path.basename(path))[0]\n",
        "              txt_path = os.path.join(IMAGE_DIRECTORY, f\"{base_filename}.txt\")\n",
        "              save_text_to_file(ocr_text, txt_path)\n",
        "            else:\n",
        "              print(\"Can't extract the text\")\n",
        "\n",
        "            # Step 2: Generate a description of the image using another model\n",
        "            desc_response = client.chat.completions.create(\n",
        "                model=DESCRIPTION_MODEL,  # model used for generating descriptions\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": DESCRIPTION_PROMPT},  # instruction prompt\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}  # same image data\n",
        "                    ]}\n",
        "                ]\n",
        "            )\n",
        "            # Extract the description result text\n",
        "            description = desc_response.choices[0].message.content\n",
        "\n",
        "            # Append the result to processed_data with image filename, OCR text, and description\n",
        "            processed_data.append({\n",
        "                \"id\": os.path.basename(path),  # filename of the image\n",
        "                \"ocr_text\": ocr_text,\n",
        "                \"description\": description\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            # If something goes wrong during processing, log the error and move to the next image\n",
        "            print(f\"Error analyzing image {path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Return the processed data in a dictionary\n",
        "    return {\"processed_data\": processed_data}\n"
      ],
      "metadata": {
        "id": "m0vh2ADgEqKw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "BrYbb5kzly-7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_with_vision_model(video_path):\n",
        "    \"\"\"\n",
        "    Conceptual function to analyze a video by extracting keyframes\n",
        "    and generating a comprehensive description using a multi-modal model.\n",
        "    \"\"\"\n",
        "    print(f\"  - Starting video analysis for {os.path.basename(video_path)}\")\n",
        "    video_description = \"\"\n",
        "\n",
        "    try:\n",
        "        # Use a library like OpenCV to read the video file\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        frame_interval = int(fps * 5)  # Sample a frame every 5 seconds\n",
        "\n",
        "        frame_count = 0\n",
        "        descriptions = []\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Process only frames at the specified interval\n",
        "            if frame_count % frame_interval == 0:\n",
        "                # Convert the frame to a format suitable for an API call\n",
        "                _, buffer = cv2.imencode('.jpg', frame)\n",
        "                encoded_image = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "                # --- This is where a real-world API call would go ---\n",
        "                # A vision model would analyze the encoded_image with a prompt like:\n",
        "                # \"Describe the key activities, text, and objects in this scene.\"\n",
        "                # The response would then be appended to descriptions.\n",
        "\n",
        "                # Placeholder for the actual model response\n",
        "                descriptions.append(f\"Scene description from video frame {frame_count}: This is a placeholder description.\")\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "        # Combine all frame descriptions into a single summary\n",
        "        video_description = \" \".join(descriptions)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video file {video_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "    return video_description"
      ],
      "metadata": {
        "id": "u98dD16klzl3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_text_and_video_node(state: IndexingState):\n",
        "    print(\"--- NODE: Analyzing Text and Video ---\")\n",
        "\n",
        "    # Get the processed data from the previous node\n",
        "    processed_data = state.get(\"processed_data\", [])\n",
        "\n",
        "    # Process text files\n",
        "    for path in tqdm(state['text_paths'], desc=\"Analyzing Text\"):\n",
        "        try:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                text_content = f.read()\n",
        "\n",
        "            # Append the text data to the processed data list\n",
        "            processed_data.append({\n",
        "                \"id\": os.path.basename(path),\n",
        "                \"text_data\": text_content,\n",
        "                \"description\": f\"Extracted text from file: {text_content}\"\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading text file {path}: {e}\")\n",
        "\n",
        "    # Process video files\n",
        "    for path in tqdm(state['video_paths'], desc=\"Analyzing Videos\"):\n",
        "        try:\n",
        "            # Use the new function to get a detailed description of the video\n",
        "            video_description = process_video_with_vision_model(path)\n",
        "\n",
        "            if video_description:\n",
        "                # Append the video description to the processed data list\n",
        "                processed_data.append({\n",
        "                    \"id\": os.path.basename(path),\n",
        "                    \"text_data\": video_description,\n",
        "                    \"description\": video_description, # Use the same description for both fields\n",
        "                    \"file_path\": path # Store the original video file path\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Warning: Could not extract useful data from video {path}.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing video {path}: {e}\")\n",
        "\n",
        "    return {\"processed_data\": processed_data}"
      ],
      "metadata": {
        "id": "tg2osnUgVMbK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_tokens=250, overlap=50):\n",
        "    \"\"\"Splits a long text into smaller chunks.\"\"\"\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=max_tokens,\n",
        "        chunk_overlap=overlap,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "AXYByoCuhqVI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_and_upsert_node(state: IndexingState):\n",
        "    print(\"--- NODE: Embedding and Upserting to Pinecone ---\")\n",
        "\n",
        "    try:\n",
        "        pc = PineconeClient(api_key=PINECONE_API_KEY)\n",
        "\n",
        "        if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
        "            pc.create_index(\n",
        "                name=PINECONE_INDEX_NAME,\n",
        "                dimension=1536,\n",
        "                metric=\"cosine\",\n",
        "                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "            )\n",
        "\n",
        "        index = pc.Index(PINECONE_INDEX_NAME)\n",
        "        vectors_to_upsert = []\n",
        "\n",
        "        # Iterate over all processed data\n",
        "        for item in tqdm(state['processed_data'], desc=\"Embedding & Uploading\"):\n",
        "            # Combine all available text data from the item\n",
        "            combined_text = \"\"\n",
        "            if \"ocr_text\" in item:\n",
        "                combined_text += f\"OCR text from image: {item['ocr_text']}. \"\n",
        "            if \"text_data\" in item:\n",
        "                combined_text += f\"Extracted text from file: {item['text_data']}. \"\n",
        "            if \"description\" in item:\n",
        "                combined_text += f\"Visual/Summary description: {item['description']}. \"\n",
        "\n",
        "            if not combined_text:\n",
        "                continue\n",
        "\n",
        "            # Chunk the combined text\n",
        "            chunks = chunk_text(combined_text)\n",
        "\n",
        "            # Generate and upsert an embedding for each chunk\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                if not \"president\" in chunk.lower():\n",
        "                  continue\n",
        "\n",
        "                if i % 100 == 0:\n",
        "                  print(f\"Working on chunk - {i} of {len(chunks)}\")\n",
        "                  chunk_id = f\"{item['id']}_chunk_{i}\"\n",
        "\n",
        "                # Check if the chunk already exists in Pinecone\n",
        "                result = index.fetch(ids=[chunk_id])\n",
        "                if result.vectors:\n",
        "                    continue\n",
        "\n",
        "                embedding = client.embeddings.create(\n",
        "                    input=[chunk],\n",
        "                    model=EMBEDDING_MODEL\n",
        "                ).data[0].embedding\n",
        "\n",
        "                vectors_to_upsert.append({\n",
        "                    \"id\": chunk_id,\n",
        "                    \"values\": embedding,\n",
        "                    \"metadata\": {\n",
        "                        \"text\": chunk,\n",
        "                        \"original_id\": item['id'],\n",
        "                        \"file_path\": item.get('file_path', os.path.join(IMAGE_DIRECTORY, item['id']))\n",
        "                    }\n",
        "                })\n",
        "\n",
        "        if vectors_to_upsert:\n",
        "            index.upsert(vectors=vectors_to_upsert, batch_size=PINECONE_BATCH_SIZE)\n",
        "            print(f\"Successfully upserted {len(vectors_to_upsert)} vectors.\")\n",
        "        else:\n",
        "            print(\"No new vectors to upsert.\")\n",
        "\n",
        "        return {}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error_message\": f\"Failed to upsert to Pinecone: {e}\"}"
      ],
      "metadata": {
        "id": "C39xf--bE5sb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_workflow = StateGraph(IndexingState)"
      ],
      "metadata": {
        "id": "oXCy4wuzFFjS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Znkn4OTbBinA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e95438-ff16-413d-fff0-9aff58519271"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b50ea07bc50>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Add the node that loads image file paths into the workflow\n",
        "indexing_workflow.add_node(\"load_paths\", load_image_paths_node)\n",
        "\n",
        "# Add the new node that loads text and video file paths\n",
        "indexing_workflow.add_node(\"load_text_and_video\", load_text_and_video_paths_node)\n",
        "\n",
        "# Add the node that analyzes the images (e.g., OCR and description)\n",
        "indexing_workflow.add_node(\"analyze_images\", analyze_images_node)\n",
        "\n",
        "# Add the new node that analyzes text and video files\n",
        "indexing_workflow.add_node(\"analyze_text_and_video\", analyze_text_and_video_node)\n",
        "\n",
        "# Add the node that generates embeddings and uploads them to Pinecone\n",
        "indexing_workflow.add_node(\"embed_and_upsert\", embed_and_upsert_node)\n",
        "\n",
        "# Define the order of execution: start with loading image paths\n",
        "indexing_workflow.add_edge(START, \"load_paths\")\n",
        "\n",
        "# After loading image paths, proceed to loading text and video paths\n",
        "indexing_workflow.add_edge(\"load_paths\", \"load_text_and_video\")\n",
        "\n",
        "# After loading text and video paths, proceed to analyzing images\n",
        "indexing_workflow.add_edge(\"load_text_and_video\", \"analyze_images\")\n",
        "\n",
        "# After analyzing images, proceed to analyzing text and video\n",
        "indexing_workflow.add_edge(\"analyze_images\", \"analyze_text_and_video\")\n",
        "\n",
        "# After analyzing text and video, proceed to embedding and upserting\n",
        "indexing_workflow.add_edge(\"analyze_text_and_video\", \"embed_and_upsert\")\n",
        "\n",
        "# Mark the final node as the end of the workflow\n",
        "indexing_workflow.add_edge(\"embed_and_upsert\", END)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the defined workflow into an executable app or pipeline\n",
        "indexing_app = indexing_workflow.compile()"
      ],
      "metadata": {
        "id": "QFH9zXYpFBOG"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log the start of the indexing pipeline execution\n",
        "print(\"--- Invoking Indexing Pipeline ---\")\n",
        "\n",
        "# Define the initial state passed into the pipeline\n",
        "initial_state = {\n",
        "    \"image_paths\": [],       # Will be populated by the 'load_paths' node\n",
        "    \"processed_data\": []     # Will be filled by later stages (analyze, embed, etc.)\n",
        "}\n",
        "\n",
        "# Invoke the compiled workflow with the initial state\n",
        "final_indexing_state = indexing_app.invoke(initial_state)\n",
        "\n",
        "# Check for any error returned in the final state and print appropriate message\n",
        "if final_indexing_state.get('error_message'):\n",
        "    print(f\"\\nIndexing failed: {final_indexing_state['error_message']}\")\n",
        "else:\n",
        "    print(\"\\n--- Indexing Pipeline Complete! ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523,
          "referenced_widgets": [
            "93d47dbf14874bd5a6df9ffba0c5a542",
            "8a0cc443064244029e5814418210b77c",
            "116db308dbd04f3aa737bf858ae67f9e",
            "49baea5843094d7184b7652b19f081b8",
            "6cc2ef34d91f4f1a9d11aeaf4da830cb",
            "eb5dc7926ae64335842df172e145a586",
            "153257e1c6e549488676eac317de0300",
            "89aced5b0765423ba7ee6ba046fb2756",
            "f9605d8012484cbb80e4335eacdf9da1",
            "b0937bd2ffdc4d50996aa31d6f794e8a",
            "e061a5e128434696bebe0897bdcf49d2"
          ]
        },
        "id": "EMecShw7FA8e",
        "outputId": "a6c41038-bad1-4065-8a52-c4b33c4edb79"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Invoking Indexing Pipeline ---\n",
            "--- NODE: Loading Image Paths ---\n",
            "Found 3 images to process.\n",
            "--- NODE: Loading Text and Video Paths ---\n",
            "Found 4 text files and 1 video files.\n",
            "--- NODE: Analyzing Images ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAnalyzing Posters:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved OCR text to ./posters/tt0084058.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAnalyzing Posters:  33%|███▎      | 1/3 [00:04<00:09,  4.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved OCR text to ./posters/tt0084867.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAnalyzing Posters:  67%|██████▋   | 2/3 [00:11<00:05,  5.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved OCR text to ./posters/tt0085121.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing Posters: 100%|██████████| 3/3 [00:16<00:00,  5.65s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NODE: Analyzing Text and Video ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing Text: 100%|██████████| 4/4 [00:00<00:00, 1960.64it/s]\n",
            "Analyzing Videos:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Starting video analysis for 1.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analyzing Videos: 100%|██████████| 1/1 [00:09<00:00,  9.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NODE: Embedding and Upserting to Pinecone ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEmbedding & Uploading:   0%|          | 0/8 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on chunk - 0 of 1542\n",
            "Working on chunk - 100 of 1542\n",
            "Working on chunk - 200 of 1542\n",
            "Working on chunk - 400 of 1542\n",
            "Working on chunk - 500 of 1542\n",
            "Working on chunk - 600 of 1542\n",
            "Working on chunk - 700 of 1542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Embedding & Uploading: 100%|██████████| 8/8 [03:47<00:00, 28.41s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upserted vectors:   0%|          | 0/742 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93d47dbf14874bd5a6df9ffba0c5a542"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully upserted 742 vectors.\n",
            "\n",
            "--- Indexing Pipeline Complete! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIcinGb0BinA"
      },
      "source": [
        "# Phase 2: The Advanced RAG Query Pipeline\n",
        "\n",
        "This is the interactive, conversational agent. It remembers chat history, validates its own search results, and provides detailed, explainable answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvxa7yn9BinA"
      },
      "source": [
        "## 6. Define the State and Nodes for the RAG Agent\n",
        "\n",
        "**Purpose**: To define the memory and capabilities of our advanced query agent. The state now includes `messages` for conversational history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3xvmhbHdBinB"
      },
      "outputs": [],
      "source": [
        "class RAGState(TypedDict):\n",
        "    \"\"\"Represents the state of our RAG (Retrieval-Augmented Generation) query workflow.\"\"\"\n",
        "\n",
        "    # A list of chat messages (e.g., previous conversation turns)\n",
        "    messages: Annotated[List[BaseMessage], add_messages]\n",
        "\n",
        "    # The current user query being processed\n",
        "    user_query: str\n",
        "\n",
        "    # Context retrieved from a vector database or knowledge base\n",
        "    retrieved_context: str\n",
        "\n",
        "    # List of image paths or identifiers retrieved alongside the context\n",
        "    retrieved_images: List[str]\n",
        "\n",
        "    # Optional error message if something fails during the workflow\n",
        "    error_message: str\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_from_pinecone_node(state: RAGState):\n",
        "    # Print a header indicating this stage of the RAG workflow\n",
        "    print(\"--- NODE: Retrieving from Pinecone ---\")\n",
        "\n",
        "    try:\n",
        "        # Initialize the Pinecone client using the API key\n",
        "        pc = PineconeClient(api_key=PINECONE_API_KEY)\n",
        "\n",
        "        # Connect to the specified Pinecone index\n",
        "        index = pc.Index(PINECONE_INDEX_NAME)\n",
        "\n",
        "        # Extract the latest user query from the chat message history\n",
        "        user_query = state['messages'][-1].content\n",
        "\n",
        "        # Generate an embedding for the user query using the embedding model\n",
        "        query_embedding = client.embeddings.create(\n",
        "            input=[user_query],\n",
        "            model=EMBEDDING_MODEL\n",
        "        ).data[0].embedding\n",
        "\n",
        "        # Query Pinecone for the top K most relevant matches using cosine similarity\n",
        "        results = index.query(\n",
        "            vector=query_embedding,\n",
        "            top_k=TOP_K_RETRIEVAL,\n",
        "            include_metadata=True  # So we can access stored text/image info\n",
        "        )\n",
        "\n",
        "        # Initialize variables to collect context and image paths\n",
        "        context = \"\"\n",
        "        image_paths = []\n",
        "\n",
        "        # Loop over each match and collect context and image path\n",
        "        for match in results['matches']:\n",
        "            context += match['metadata']['text'] + \"\\n---\\n\"  # Append retrieved text with separator\n",
        "            image_paths.append(match['metadata']['file_path'])  # Track associated image file\n",
        "\n",
        "        # Log how many posters were retrieved\n",
        "        print(f\"Retrieved {len(image_paths)} relevant posters.\")\n",
        "\n",
        "        # Return the updated state for the next node in the workflow\n",
        "        return {\n",
        "            \"retrieved_context\": context,\n",
        "            \"retrieved_images\": image_paths,\n",
        "            \"user_query\": user_query\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return an error message if anything goes wrong during retrieval\n",
        "        return {\"error_message\": f\"Failed to retrieve from Pinecone: {e}\"}\n"
      ],
      "metadata": {
        "id": "j63D5VYgGYet"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_retrieval_node(state: RAGState):\n",
        "    \"\"\"Self-correction node to check if the retrieval was successful.\"\"\"\n",
        "\n",
        "    # Print header for this validation step\n",
        "    print(\"--- NODE: Validating Retrieval ---\")\n",
        "\n",
        "    # Check if any context was retrieved in the previous step\n",
        "    if not state.get('retrieved_context'):\n",
        "        # Log failure and return a fallback AI message to the user\n",
        "        print(\"Validation failed: No context was retrieved.\")\n",
        "        ai_message = AIMessage(content=\"I couldn't find any relevant posters for your query. Could you please rephrase it or ask something else?\")\n",
        "        return {\"messages\": [ai_message]}\n",
        "    else:\n",
        "        # Log success and allow workflow to continue\n",
        "        print(\"Validation successful: Context retrieved.\")\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "lIzvBMMXGcXk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_node(state: RAGState):\n",
        "    # Print a header indicating the final answer generation step\n",
        "    print(\"--- NODE: Generating Final Answer ---\")\n",
        "\n",
        "    try:\n",
        "        # Combine previous chat messages into a history string (excluding the latest user message)\n",
        "        history = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in state['messages'][:-1]])\n",
        "\n",
        "        # Construct the prompt by combining the RAG instructions, chat history, retrieved context, and current query\n",
        "        prompt = f\"{RAG_PROMPT}\\n\\nCHAT HISTORY:\\n{history}\\n\\nRETRIEVED CONTEXT:\\n{state['retrieved_context']}\\n\\nCURRENT USER QUESTION: {state['user_query']}\"\n",
        "\n",
        "        # Generate a response from the RAG model using the constructed prompt\n",
        "        response = client.chat.completions.create(\n",
        "            model=RAG_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        # Extract the answer text from the model's response\n",
        "        answer = response.choices[0].message.content\n",
        "\n",
        "        # Wrap the answer in an AIMessage object\n",
        "        ai_message = AIMessage(content=answer)\n",
        "\n",
        "        # Return the AI's response to be added to the conversation\n",
        "        return {\"messages\": [ai_message]}\n",
        "\n",
        "    except Exception as e:\n",
        "        # Return error message in case of failure\n",
        "        return {\"error_message\": f\"Failed to generate answer: {e}\"}\n"
      ],
      "metadata": {
        "id": "rqN3lBF4Gd0S"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpOdYlSsBinB"
      },
      "source": [
        "## 7. Construct and Run the RAG Query Graph\n",
        "\n",
        "**Purpose**: To build the conversational agent, now including a conditional edge for the self-correction loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tZh0mk5yBinB"
      },
      "outputs": [],
      "source": [
        "def route_after_validation(state: RAGState):\n",
        "    \"\"\"If validation fails, end the workflow. Otherwise, generate an answer.\"\"\"\n",
        "\n",
        "    # If no context was retrieved during validation, terminate the workflow\n",
        "    if not state.get('retrieved_context'):\n",
        "        return END\n",
        "\n",
        "    # Otherwise, proceed to generate the final answer\n",
        "    return \"generate_answer\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a SqliteSaver instance for saving memory/state using an SQLite database connection\n",
        "memory = SqliteSaver(conn=sqlite3.connect(\"rag_agent.sqlite\", check_same_thread=False))\n"
      ],
      "metadata": {
        "id": "StzHH75tGmMB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new StateGraph workflow using the RAGState TypedDict to track state\n",
        "rag_workflow = StateGraph(RAGState)\n"
      ],
      "metadata": {
        "id": "kO-8FnvBGnwn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the retrieval node to the RAG workflow\n",
        "rag_workflow.add_node(\"retrieve\", retrieve_from_pinecone_node)\n",
        "\n",
        "# Add the retrieval validation node to check if relevant data was found\n",
        "rag_workflow.add_node(\"validate_retrieval\", validate_retrieval_node)\n",
        "\n",
        "# Add the final answer generation node that creates the AI response\n",
        "rag_workflow.add_node(\"generate_answer\", generate_answer_node)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWvRbqjyGqmh",
        "outputId": "8179d52b-dc1c-432c-cdea-14c439c85850"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b50e80ade90>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the workflow starting point leading to the retrieval node\n",
        "rag_workflow.add_edge(START, \"retrieve\")\n",
        "\n",
        "# After retrieval, proceed to validate the retrieval results\n",
        "rag_workflow.add_edge(\"retrieve\", \"validate_retrieval\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGQ9MjOOGsJt",
        "outputId": "0fc5aa77-099a-4223-b876-a779ffb13e7b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b50e80ade90>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add conditional edges from the 'validate_retrieval' node based on routing function outcome\n",
        "rag_workflow.add_conditional_edges(\n",
        "    \"validate_retrieval\",\n",
        "    route_after_validation,  # function that decides the next node or end\n",
        "    {\n",
        "        \"generate_answer\": \"generate_answer\",  # if validation passes, go to generate_answer\n",
        "        END: END  # if validation fails, end the workflow\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndoGmh1NGuVi",
        "outputId": "a435535b-1618-47c9-cc0f-fafe3b28457e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b50e80ade90>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect the 'generate_answer' node to the end of the workflow, marking completion\n",
        "rag_workflow.add_edge(\"generate_answer\", END)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3AKkhLsGvwp",
        "outputId": "c2c91bea-3523-4b43-c695-2f9c44c9fb4a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7b50e80ade90>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the RAG workflow into an executable app, using the memory checkpointer for state persistence\n",
        "rag_app = rag_workflow.compile(checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "KhGwpE3WGxPl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the correct name, IPImage, for displaying in the notebook\n",
        "display(IPImage(rag_app.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Crx4vJu-GzNN",
        "outputId": "91df4fd1-b658-4452-eaee-8a1c3a83a815"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx6RijMSBinB"
      },
      "source": [
        "## 8. Chat with your RAG Agent!\n",
        "\n",
        "**Purpose**: This is the final, interactive part. You can run this cell multiple times to have a conversation with the agent. It will remember your previous questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yaxIdqfUBinD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc2ed92f-91aa-4720-a762-a9ff80462900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question about the movie posters (or type 'exit' to quit): What details you have about presidents\n",
            "\n",
            "--- Invoking Agent ---\n",
            "--- NODE: Retrieving from Pinecone ---\n",
            "Retrieved 3 relevant posters.\n",
            "--- NODE: Validating Retrieval ---\n",
            "Validation successful: Context retrieved.\n",
            "--- NODE: Generating Final Answer ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Agent's Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It seems like you are asking about details related to United States presidents. Here's an overview:\n\n1. **Role and Responsibilities**: The President of the United States is the head of state and head of government of the U.S. The president directs the executive branch of the federal government and is the commander-in-chief of the United States Armed Forces.\n\n2. **Election and Term**: The president is elected for a four-year term through an Electoral College and may serve a maximum of two terms.\n\n3. **Powers and Duties**: The president has the power to sign legislation into law or veto bills passed by Congress. The president also appoints federal executive, diplomatic, regulatory, and judicial officers, and concludes treaties, with Senate approval.\n\n4. **Presidential Immunity and Protection**: There is some debate around the extent of presidential immunity from court cases, particularly while in office. After leaving office, former presidents have certain protections and benefits, like pensions, travel funds, and franking privileges. Additionally, former presidents, their spouses, and their children until age 16 used to have personal protection.\n\nIf you have specific aspects about U.S. presidents or are looking for something else, feel free to ask!"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Retrieved Posters:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the movie posters (or type 'exit' to quit): I need to know about USA Presidents\n",
            "\n",
            "--- Invoking Agent ---\n",
            "--- NODE: Retrieving from Pinecone ---\n",
            "Retrieved 3 relevant posters.\n",
            "--- NODE: Validating Retrieval ---\n",
            "Validation successful: Context retrieved.\n",
            "--- NODE: Generating Final Answer ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Agent's Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "It looks like you're interested in information about the Presidents of the United States. Here's a broad overview:\n\n1. **Role and Responsibilities**: The President of the United States is both the head of state and the head of government. This position oversees the executive branch of the federal government and acts as the commander-in-chief of the United States Armed Forces.\n\n2. **Election and Term**: Presidents are elected to serve a four-year term via the Electoral College and are eligible to serve a maximum of two terms, as per the 22nd Amendment to the Constitution.\n\n3. **Powers and Duties**: The President has the authority to sign or veto legislation, appoint federal officers, diplomats, and judges, and negotiate treaties (with the Senate's advice and consent). The President also plays a significant role in setting and implementing domestic and foreign policy.\n\n4. **Presidential Immunity and Protection**: While in office, there is ongoing debate about the extent of a president's legal immunities. After leaving office, former presidents receive certain privileges and benefits, such as pensions, travel allowances, and mailing privileges (known as franking). Also, former presidents and their immediate families are provided protection, a provision that has evolved over the years.\n\nIf you have specific questions or need more detailed information about a particular president or aspect of the presidency, feel free to ask!"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Retrieved Posters:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the movie posters (or type 'exit' to quit): Who is the last USA President?\n",
            "\n",
            "--- Invoking Agent ---\n",
            "--- NODE: Retrieving from Pinecone ---\n",
            "Retrieved 3 relevant posters.\n",
            "--- NODE: Validating Retrieval ---\n",
            "Validation successful: Context retrieved.\n",
            "--- NODE: Generating Final Answer ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Agent's Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The current and most recent President of the United States, as of October 2023, is Joe Biden. He has been serving as the 46th president since January 20, 2021."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Retrieved Posters:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the movie posters (or type 'exit' to quit): What details you have about Donald Trump\n",
            "\n",
            "--- Invoking Agent ---\n",
            "--- NODE: Retrieving from Pinecone ---\n",
            "Retrieved 3 relevant posters.\n",
            "--- NODE: Validating Retrieval ---\n",
            "Validation successful: Context retrieved.\n",
            "--- NODE: Generating Final Answer ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Agent's Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Donald Trump served as the 45th President of the United States from January 20, 2017, to January 20, 2021. Here's a brief overview of his presidency and background:\n\n1. **Background**: Prior to his presidency, Donald Trump was a businessman and television personality. He was involved in real estate development and was the owner of the Trump Organization, which has interests in real estate, hotels, and golf courses. He gained widespread fame through the reality TV show \"The Apprentice.\"\n\n2. **Election**: Trump ran as the Republican candidate in the 2016 presidential election and won against Democratic candidate Hillary Clinton. His victory was considered surprising by many analysts and was attributed in part to his appeal to disaffected working-class voters.\n\n3. **Presidency**:\n   - **Domestic Policy**: Trump's presidency was marked by significant policy changes, including tax reform legislation (the Tax Cuts and Jobs Act of 2017). He focused on deregulating industries and appointed conservative judges to the federal judiciary, including three Supreme Court justices.\n   - **Immigration**: Trump took a strong stance on immigration, advancing policies including the construction of a border wall between the United States and Mexico and implementing travel bans affecting several predominantly Muslim countries.\n   - **Foreign Policy**: His foreign policy was characterized by the \"America First\" doctrine, which criticized globalism. Trump's administration renegotiated trade agreements, such as the United States-Mexico-Canada Agreement (USMCA), and took a hardline stance against countries like China and Iran.\n   - **Impeachments**: Donald Trump was impeached twice by the House of Representatives. The first impeachment in 2019 was related to the Ukraine investigation, and the second in 2021 followed the January 6 Capitol riot. He was acquitted by the Senate both times.\n\n4. **Post-Presidency**: After leaving office, Trump has remained a significant figure in American politics, continuing to have substantial influence over the Republican Party.\n\nPlease let me know if there are specific aspects about Donald Trump you would like more information on!"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Retrieved Posters:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the movie posters (or type 'exit' to quit): quit\n",
            "\n",
            "--- Invoking Agent ---\n",
            "--- NODE: Retrieving from Pinecone ---\n",
            "Retrieved 3 relevant posters.\n",
            "--- NODE: Validating Retrieval ---\n",
            "Validation successful: Context retrieved.\n",
            "--- NODE: Generating Final Answer ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Agent's Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "If you're looking to exit the conversation, you can simply close the chat or let me know if there's anything else you need before leaving! If you have questions in the future, feel free to reach out."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Retrieved Posters:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the movie posters (or type 'exit' to quit): quit\n",
            "\n",
            "--- Invoking Agent ---\n",
            "--- NODE: Retrieving from Pinecone ---\n",
            "Retrieved 3 relevant posters.\n",
            "--- NODE: Validating Retrieval ---\n",
            "Validation successful: Context retrieved.\n",
            "--- NODE: Generating Final Answer ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Agent's Answer"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "If you're looking to exit the conversation, you can simply close the chat or let me know if there's anything else you need before leaving! If you have questions in the future, feel free to reach out."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n**Retrieved Posters:**"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "\n",
            "Ask a question about the movie posters (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "# Generate a unique thread ID for this conversation session\n",
        "thread_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "\n",
        "while True:\n",
        "    # Prompt user for a question or exit command\n",
        "    user_input = input(\"Ask a question about the movie posters (or type 'exit' to quit): \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break  # Exit the loop and end the program\n",
        "\n",
        "    print(\"\\n--- Invoking Agent ---\")\n",
        "\n",
        "    # Prepare the initial input with the user's message wrapped as a HumanMessage\n",
        "    initial_input = {\"messages\": [HumanMessage(content=user_input)]}\n",
        "\n",
        "    # Stream the response from the RAG agent app\n",
        "    events = rag_app.stream(\n",
        "        initial_input,\n",
        "        thread_config,\n",
        "        stream_mode=\"values\",  # Stream output values as they arrive\n",
        "    )\n",
        "\n",
        "    final_state = None\n",
        "    # Iterate over streamed events to get the final state after completion\n",
        "    for event in events:\n",
        "        final_state = event\n",
        "\n",
        "    if final_state:\n",
        "        # Extract the final answer text from the last message\n",
        "        final_answer = final_state[\"messages\"][-1].content\n",
        "\n",
        "        # Display the AI's answer formatted as Markdown\n",
        "        display(Markdown(\"### Agent's Answer\"))\n",
        "        display(Markdown(final_answer))\n",
        "\n",
        "        # If there are retrieved images, display them below the answer\n",
        "        if final_state.get(\"retrieved_images\"):\n",
        "            display(Markdown(\"\\n**Retrieved Posters:**\"))\n",
        "            for img_path in final_state['retrieved_images']:\n",
        "                try:\n",
        "                    # Attempt to display each retrieved image with width=200px\n",
        "                    #display(IPImage(filename=img_path, width=200))\n",
        "                    pass\n",
        "                except Exception as e:\n",
        "                    # Handle any errors loading or displaying the image\n",
        "                    print(f\"Could not display image {img_path}: {e}\")\n",
        "\n",
        "    # Print a visual separator before the next query\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BC7lF26ZStLT"
      },
      "execution_count": 39,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93d47dbf14874bd5a6df9ffba0c5a542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a0cc443064244029e5814418210b77c",
              "IPY_MODEL_116db308dbd04f3aa737bf858ae67f9e",
              "IPY_MODEL_49baea5843094d7184b7652b19f081b8"
            ],
            "layout": "IPY_MODEL_6cc2ef34d91f4f1a9d11aeaf4da830cb"
          }
        },
        "8a0cc443064244029e5814418210b77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb5dc7926ae64335842df172e145a586",
            "placeholder": "​",
            "style": "IPY_MODEL_153257e1c6e549488676eac317de0300",
            "value": "Upserted vectors: 100%"
          }
        },
        "116db308dbd04f3aa737bf858ae67f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89aced5b0765423ba7ee6ba046fb2756",
            "max": 742,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9605d8012484cbb80e4335eacdf9da1",
            "value": 742
          }
        },
        "49baea5843094d7184b7652b19f081b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0937bd2ffdc4d50996aa31d6f794e8a",
            "placeholder": "​",
            "style": "IPY_MODEL_e061a5e128434696bebe0897bdcf49d2",
            "value": " 742/742 [00:26&lt;00:00, 112.70it/s]"
          }
        },
        "6cc2ef34d91f4f1a9d11aeaf4da830cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb5dc7926ae64335842df172e145a586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "153257e1c6e549488676eac317de0300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89aced5b0765423ba7ee6ba046fb2756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9605d8012484cbb80e4335eacdf9da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0937bd2ffdc4d50996aa31d6f794e8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e061a5e128434696bebe0897bdcf49d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}