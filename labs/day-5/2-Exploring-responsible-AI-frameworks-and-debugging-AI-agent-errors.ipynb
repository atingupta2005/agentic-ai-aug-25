{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d310417",
   "metadata": {},
   "source": [
    "# üõ† Lab: Exploring Responsible AI Frameworks & Debugging AI Agent Errors  \n",
    "\n",
    "This hands-on guide shows you how to  \n",
    "1. Integrate open-source Responsible AI toolkits into your agent workflow  \n",
    "2. Evaluate and mitigate bias in a simple classification agent  \n",
    "3. Add logging, tracing, and error-monitoring to catch and fix agent failures  \n",
    "\n",
    "---\n",
    "\n",
    "## Lab Objectives  \n",
    "\n",
    "- Install and configure two Responsible AI frameworks: IBM AIF360 and Fairlearn  \n",
    "- Build a toy classification agent (e.g., loan approval) and measure fairness metrics  \n",
    "- Use the Google What-If Tool in Jupyter to interactively explore model behavior  \n",
    "- Instrument your LangChain agent with callbacks, Python logging, and Sentry for real-time error monitoring  \n",
    "- Simulate common agent errors (API timeouts, missing tool, hallucinations) and practice debugging  \n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites  \n",
    "\n",
    "- Python 3.8+  \n",
    "- Jupyter Notebook or Lab  \n",
    "- An OpenAI API key (export as `OPENAI_API_KEY`)  \n",
    "- (Optional) A Sentry account DSN for error monitoring  \n",
    "- Basic familiarity with Python, pandas, scikit-learn, and LangChain  \n",
    "\n",
    "Install the core packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf94a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --upgrade \\\n",
    "  scikit-learn pandas numpy matplotlib seaborn \\\n",
    "  aif360 fairlearn \\\n",
    "  google-what-if-tool langchain openai \\\n",
    "  sentry-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a60ccd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Responsible AI Frameworks Overview  \n",
    "\n",
    "### 1.1 IBM AI Fairness 360 (AIF360)  \n",
    "\n",
    "AIF360 provides metrics and algorithms to detect and mitigate bias.  \n",
    "\n",
    "#### 1.1.1 Quickstart in Jupyter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e0318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook cell\n",
    "from aif360.datasets import AdultDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "\n",
    "# Load ‚ÄúAdult‚Äù census dataset\n",
    "data = AdultDataset(protected_attribute_names=['sex'], \n",
    "                    privileged_classes=[['Male']])\n",
    "metric = BinaryLabelDatasetMetric(data, \n",
    "                                  unprivileged_groups=[{'sex': 0}],\n",
    "                                  privileged_groups=[{'sex': 1}])\n",
    "\n",
    "print(\"Disparate impact:\", metric.disparate_impact())\n",
    "print(\"Statistical parity difference:\", metric.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8e6ca",
   "metadata": {},
   "source": [
    "- **Disparate Impact < 0.8** signals potential bias.  \n",
    "- **Statistical Parity Difference** ‚â† 0 indicates imbalance.\n",
    "\n",
    "#### 1.1.2 Mitigation: Reweighing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61097147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "RW = Reweighing(unprivileged_groups=[{'sex':0}],\n",
    "                privileged_groups=[{'sex':1}])\n",
    "data_transf = RW.fit_transform(data)\n",
    "\n",
    "# Check metrics again\n",
    "metric_transf = BinaryLabelDatasetMetric(data_transf,\n",
    "                                         unprivileged_groups=[{'sex':0}],\n",
    "                                         privileged_groups=[{'sex':1}])\n",
    "print(\"Post-Reweighing disparate impact:\", metric_transf.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebf4e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1.2 Fairlearn  \n",
    "\n",
    "Fairlearn offers fairness metrics and post-processing algorithms.\n",
    "\n",
    "#### 1.2.1 Quickstart in Jupyter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook cell\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, accuracy_score\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd\n",
    "X = pd.DataFrame(data.features, columns=data.feature_names)\n",
    "y = pd.Series(data.labels.ravel())\n",
    "\n",
    "# Train a logistic model\n",
    "clf = LogisticRegression(max_iter=1000).fit(X, y)\n",
    "\n",
    "# Compute metrics by group\n",
    "metric_frame = MetricFrame(\n",
    "    metrics={'accuracy': accuracy_score, 'selection_rate': selection_rate},\n",
    "    y_true=y, y_pred=clf.predict(X),\n",
    "    sensitive_features=data.protected_attributes_df['sex']\n",
    ")\n",
    "print(metric_frame.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23898e3",
   "metadata": {},
   "source": [
    "#### 1.2.2 Mitigation: ThresholdOptimizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd6f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TO = ThresholdOptimizer(\n",
    "    estimator=clf,\n",
    "    constraints=\"demographic_parity\",\n",
    "    predict_method='predict_proba',\n",
    "    prefit=True\n",
    ")\n",
    "TO.fit(X, y, sensitive_features=data.protected_attributes_df['sex'])\n",
    "y_pred_balanced = TO.predict(X, sensitive_features=data.protected_attributes_df['sex'])\n",
    "\n",
    "mf_bal = MetricFrame(\n",
    "    metrics=accuracy_score,\n",
    "    y_true=y, y_pred=y_pred_balanced,\n",
    "    sensitive_features=data.protected_attributes_df['sex']\n",
    ")\n",
    "print(\"Balanced accuracy by sex:\\n\", mf_bal.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d692d2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Interactive Exploration with the What-If Tool  \n",
    "\n",
    "### 2.1 Launch What-If in Jupyter  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1001569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook cell\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
    "\n",
    "# Sample 100 rows\n",
    "sample_df = X.sample(100, random_state=0)\n",
    "builder = WitConfigBuilder(sample_df, \n",
    "                           label_vocab=[\"‚â§50K\",\" >50K\"]) \\\n",
    "  .set_model_type(\"classification\") \\\n",
    "  .set_model_predict_function(lambda inputs: clf.predict_proba(inputs))\n",
    "WitWidget(builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc01d24",
   "metadata": {},
   "source": [
    "- Inspect per-instance fairness  \n",
    "- Explore counterfactuals (‚ÄúWhat if the same person were Male vs. Female?‚Äù)  \n",
    "- Visualize decision boundary changes  \n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Building & Evaluating a Classification Agent  \n",
    "\n",
    "### 3.1 Agent Workflow  \n",
    "\n",
    "- **Sense:** Accept applicant data (age, income, sex, etc.)  \n",
    "- **Plan:** Call `clf.predict` or `TO.predict` for approval  \n",
    "- **Act:** Return ‚ÄúApproved‚Äù / ‚ÄúDenied‚Äù  \n",
    "- **Observe:** Log outcome and fairness metrics  \n",
    "\n",
    "### 3.2 Code: Simple LangChain Agent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe94d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: fair_agent.py\n",
    "import os, logging\n",
    "from langchain import Tool, initialize_agent, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"FairAgent\")\n",
    "\n",
    "# Load pre-trained or retrain for demo\n",
    "data = AdultDataset().convert_to_dataframe()[0]\n",
    "X = data.drop(columns=['income-per-year'])\n",
    "y = (data['income-per-year'] == \">50K\").astype(int)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000).fit(X, y)\n",
    "TO = ThresholdOptimizer(estimator=clf,\n",
    "                        constraints=\"demographic_parity\",\n",
    "                        predict_method='predict_proba',\n",
    "                        prefit=True)\n",
    "\n",
    "def classify_loan(payload: dict) -> dict:\n",
    "    # payload example: {\"age\": 40, \"sex\": \"Female\", ...}\n",
    "    df = pd.DataFrame([payload])\n",
    "    approved = TO.predict(df, sensitive_features=df[\"sex\"])\n",
    "    logger.info(f\"Input: {payload}, Approved: {approved[0]}\")\n",
    "    return {\"approved\": bool(approved[0])}\n",
    "\n",
    "loan_tool = Tool(\n",
    "    name=\"classify_loan\",\n",
    "    func=classify_loan,\n",
    "    description=\"Classify loan applications with fairness postprocessing.\"\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "agent = initialize_agent(\n",
    "    tools=[loan_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(agent.run(\"Classify this applicant: age 30, sex Female, income 40K, education Bachelors.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb4c52",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate Agent Fairness in Batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook cell\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "\n",
    "# Generate predictions\n",
    "df_X = X.sample(500)\n",
    "payloads = df_X.to_dict(orient=\"records\")\n",
    "results = [classify_loan(p)['approved'] for p in payloads]\n",
    "mf = MetricFrame(\n",
    "    metrics=selection_rate,\n",
    "    y_true=(y.loc[df_X.index] == 1).astype(int),\n",
    "    y_pred=results,\n",
    "    sensitive_features=df_X['sex']\n",
    ")\n",
    "print(\"Agent approval rates:\\n\", mf.by_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c13c86",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Debugging AI Agent Errors  \n",
    "\n",
    "### 4.1 Common Error Scenarios  \n",
    "\n",
    "- **API Timeouts** (LLM or external tool)  \n",
    "- **Tokenization Errors** (invalid input shape)  \n",
    "- **Missing Tool Invocation** (agent fails to pick correct tool)  \n",
    "- **Model Hallucinations** (LLM returns incorrect data)  \n",
    "\n",
    "### 4.2 Python Logging & Verbose Mode  \n",
    "\n",
    "- Use Python‚Äôs `logging` at INFO/DEBUG level  \n",
    "- In LangChain: pass `verbose=True` to trace calls  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"langchain\").setLevel(logging.DEBUG)\n",
    "agent = initialize_agent(..., verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da18e37",
   "metadata": {},
   "source": [
    "### 4.3 LangChain Callbacks & Tracers  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a981c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "tracer = LangChainTracer()\n",
    "manager = CallbackManager([tracer])\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[loan_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    callback_manager=manager,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# After a run:\n",
    "for span in tracer.get_finished_spans():\n",
    "    print(span.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91128ee1",
   "metadata": {},
   "source": [
    "- Inspect each LLM call, prompt, response, and tool call  \n",
    "- Replay and diagnose where the logic deviated  \n",
    "\n",
    "### 4.4 Sentry Integration for Runtime Monitoring  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at top of your script\n",
    "import sentry_sdk\n",
    "sentry_sdk.init(dsn=os.getenv(\"SENTRY_DSN\"), traces_sample_rate=1.0)\n",
    "\n",
    "# In classify_loan or agent code\n",
    "try:\n",
    "    # classification logic...\n",
    "except Exception as e:\n",
    "    sentry_sdk.capture_exception(e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bbd348",
   "metadata": {},
   "source": [
    "- View exceptions, stack traces, and request context in Sentry dashboard  \n",
    "\n",
    "### 4.5 Interactive Debugging in Jupyter  \n",
    "\n",
    "- Use `%debug` magic after an exception to step into code  \n",
    "- Insert `breakpoint()` calls in your agent functions  \n",
    "- Inspect local variables, call stack, and tool inputs  \n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Simulating and Fixing Errors  \n",
    "\n",
    "1. **Simulate a Timeout:**  \n",
    "   ```python\n",
    "   import time\n",
    "   def flaky_tool(x): \n",
    "       time.sleep(10)  # exceed default timeout\n",
    "       return \"done\"\n",
    "   ```\n",
    "   - Observe how agent hangs; add `timeout` in requests or LLM call.\n",
    "\n",
    "2. **Simulate Missing Tool:**  \n",
    "   ```python\n",
    "   tools = []  # no tools passed\n",
    "   # agent.run will raise an error that no tool can handle the request\n",
    "   ```\n",
    "   - Catch and provide a fallback in your agent loop.\n",
    "\n",
    "3. **Hallucination Handling:**  \n",
    "   - Post-process LLM output with simple validators  \n",
    "   - e.g., ensure classification outputs only ‚ÄúTrue‚Äù/‚ÄúFalse‚Äù  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
