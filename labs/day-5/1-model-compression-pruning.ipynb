{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2quYYr7sUPz"
      },
      "source": [
        "# Model Pruning with a Pre-Trained Transformer\n",
        "1.  Load a pre-trained sentiment analysis model (`DistilBERT`).\n",
        "2.  Evaluate its performance on a sample sentence **before** pruning.\n",
        "3.  Inspect the model's layers to choose a target for pruning.\n",
        "4.  Apply **unstructured magnitude pruning** to the target layer.\n",
        "     - Unstructured magnitude pruning is a popular technique for compressing large language models (LLMs) by removing individual connections, or weights, that are deemed least important\n",
        "5.  Verify the effect of pruning by measuring the layer's sparsity.\n",
        "     - The percentage of zero-valued parameters\n",
        "6.  Evaluate the model's performance **after** pruning to observe the impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUdEE1ZZsUP3"
      },
      "source": [
        "---\n",
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-p_SQgrKsUP4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers torch numpy huggingface_hub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IdRItfiK2f9X"
      },
      "outputs": [],
      "source": [
        "!pip install 'accelerate>=0.26.0' --upgrade -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6HErPljsUP6"
      },
      "source": [
        "---\n",
        "## 2. Import Libraries\n",
        "We'll import all the necessary components for our task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E1evyVmzsUP7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.utils.prune as prune\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJdEZAkjsUP9"
      },
      "source": [
        "---\n",
        "## 3. Define Model and Device\n",
        "Let's specify the model we'll use from the Hugging Face Hub and set up our compute device (GPU if available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "25VsnpGAsUP-"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tbjpb4UEsUQB"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGgRG9QpsUQC",
        "outputId": "dcdd3c88-0bf1-48a4-e8ab-8274c5cb89f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-N8nZSMsUQC"
      },
      "source": [
        "---\n",
        "## 4. Load Pre-Trained Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcOn0Q9esUQD",
        "outputId": "6288af94-4b27-49b1-fde2-482767592312"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RhJFcRDLsUQD"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSCGyBV8sUQE",
        "outputId": "cf15a802-43ae-4b6d-97ab-754f6616488e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-ABrxM-sUQE"
      },
      "source": [
        "---\n",
        "## 5. Evaluate the Original Model's Performance\n",
        "Before we change anything, let's establish a baseline for the model's performance on a sample sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e7AluFksUQF"
      },
      "source": [
        "### Create a helper function for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "do9E2A5NsUQF"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer, sentence, device):\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    probabilities = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "    predicted_class_id = torch.argmax(logits, dim=1).item()\n",
        "    prediction = model.config.id2label[predicted_class_id]\n",
        "\n",
        "    print(f'Sentence: \"{sentence}\"')\n",
        "    print(f'Prediction: {prediction} (Confidence: {probabilities[predicted_class_id]:.4f})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxfK0I_jsUQF"
      },
      "outputs": [],
      "source": [
        "test_sentences = [\n",
        "    'The performances in this movie are absolutely stellar',\n",
        "    'The movie was not bad.',\n",
        "    \"I'm not sure if I liked the new design.\",\n",
        "    'The food was, well, edible.',\n",
        "    \"It's an interesting idea, but I'm not sold.\",\n",
        "    'The book had its moments.',\n",
        "    'I have mixed feelings about the new policy.',\n",
        "    'The performance was...memorable.',\n",
        "    'That\\'s one way to look at it.',\n",
        "    'The weather is certainly something.',\n",
        "    \"I wouldn't say I loved it, but I didn't hate it either.\",\n",
        "    'The acting was surprisingly good in an otherwise mediocre film.',\n",
        "    'The service was incredibly slow, but the food was worth the wait.',\n",
        "    \"It's a bold strategy, Cotton. Let's see if it pays off for 'em.\",\n",
        "    'This is a very unique piece of art.',\n",
        "    'The ending of the show was definitely a choice.',\n",
        "    'I could see why some people would like this.',\n",
        "    'The special effects were amazing, but the plot was a bit weak.',\n",
        "    'Well, that was an experience.',\n",
        "    'The new update is... different.',\n",
        "    \"I'm on the fence about this one.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSJJewEosUQF",
        "outputId": "fc8c6ef9-c8d1-4c4f-f11c-8845e3c063c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluating Original Model ---\n",
            "Sentence: \"The performances in this movie are absolutely stellar.\"\n",
            "Prediction: POSITIVE (Confidence: 0.9999)\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Evaluating Original Model ---\")\n",
        "\n",
        "for test_sentence in test_sentences:\n",
        " evaluate_model(model, tokenizer, test_sentence, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgSxPhkQsUQG"
      },
      "source": [
        "---\n",
        "## 6. The Pruning Process\n",
        "Now we'll perform the actual pruning. We'll target the **query projection layer** (`q_lin`) in the first attention block of the transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJGhSVUNsUQG"
      },
      "source": [
        "### Step 6.1: Select the Target Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_TpHjxbwsUQG"
      },
      "outputs": [],
      "source": [
        "module_to_prune = model.distilbert.transformer.layer[0].attention.q_lin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKjuADKksUQG",
        "outputId": "bbe7f7e9-a957-41a1-f852-060e537fedc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected module for pruning:\n",
            "Linear(in_features=768, out_features=768, bias=True)\n"
          ]
        }
      ],
      "source": [
        "print(\"Selected module for pruning:\")\n",
        "print(module_to_prune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JONMz2mhsUQG"
      },
      "source": [
        "### Step 6.2: Check Sparsity Before Pruning\n",
        "Sparsity is the percentage of weights that are zero. For an unpruned model, this should be 0%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mJoKnP1-sUQH"
      },
      "outputs": [],
      "source": [
        "def calculate_sparsity(module):\n",
        "    return 100. * float(torch.sum(module.weight == 0)) / float(module.weight.nelement())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IU_PFRYLsUQH",
        "outputId": "b8ec4c86-b411-4f73-d997-4d797a33ecc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity before pruning: 0.00%\n"
          ]
        }
      ],
      "source": [
        "initial_sparsity = calculate_sparsity(module_to_prune)\n",
        "print(f\"Sparsity before pruning: {initial_sparsity:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YOw71eIsUQH"
      },
      "source": [
        "### Step 6.3: Apply Pruning\n",
        "We will prune 30% of the weights in the layer with the lowest L1 magnitude (i.e., closest to zero)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq1iGVdxsUQH",
        "outputId": "aadcbb1e-1824-4199-fcdc-9ad4553591ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=768, bias=True)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prune.l1_unstructured(module_to_prune, name=\"weight\", amount=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UxvRMUEsUQI"
      },
      "source": [
        "### Step 6.4: Check Sparsity After Pruning\n",
        "The pruning is applied via a 'forward hook'. The original weights are still there, but a mask is applied. The sparsity calculation should now reflect the pruned weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfuSq9H-sUQI",
        "outputId": "3724d0ed-8056-4e9e-b721-562de1475e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity after applying pruning mask: 30.00%\n"
          ]
        }
      ],
      "source": [
        "sparsity_after_pruning = calculate_sparsity(module_to_prune)\n",
        "print(f\"Sparsity after applying pruning mask: {sparsity_after_pruning:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n9OAP4RsUQI"
      },
      "source": [
        "### Step 6.5: Make the Pruning Permanent\n",
        "The `prune.remove` function removes the hook and permanently sets the pruned weights to zero in the weight tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GMqyh-TsUQJ",
        "outputId": "e3f463b2-a302-4a9c-d964-5265ce9c972c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=768, out_features=768, bias=True)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prune.remove(module_to_prune, 'weight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8vRErY6sUQJ",
        "outputId": "40181b84-49fe-40ba-feff-8ff3359a04d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity after making pruning permanent: 30.00%\n"
          ]
        }
      ],
      "source": [
        "final_sparsity = calculate_sparsity(module_to_prune)\n",
        "print(f\"Sparsity after making pruning permanent: {final_sparsity:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wffyqmq5sUQJ"
      },
      "source": [
        "---\n",
        "## 7. Evaluate the Pruned Model (Before Fine-Tuning)\n",
        "Now let's see how our model performs on the same sentence after we've removed 30% of the weights from a key layer. We expect a drop in performance or confidence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ6YfYNgsUQK",
        "outputId": "a3448a6a-c493-457c-d15d-0328299a47d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluating Pruned Model ---\n",
            "Sentence: \"The performances in this movie are absolutely stellar.\"\n",
            "Prediction: POSITIVE (Confidence: 0.9999)\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Evaluating Pruned Model ---\")\n",
        "evaluate_model(model, tokenizer, test_sentence, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0aedb623b22d483a8a2d0d26c89c9dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e882adc024b414aaa9fc39ace735653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36c89c53897e45628ee8b8596a131f71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57ec5fed78f7467688526565e14d0559": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71eb642f30c6460bb05f4dfdb7e6fbb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aedb623b22d483a8a2d0d26c89c9dd4",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5ffbc3f46f84cc3a7d773a4e66d6023",
            "value": 100
          }
        },
        "7a8c05ebe25344baa7dbf4bbf31c3bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92bab78287e6445599c3d24a114c7fcd",
            "placeholder": "​",
            "style": "IPY_MODEL_e823db771edb4f6e82fd72e0f081beec",
            "value": "Map: 100%"
          }
        },
        "92bab78287e6445599c3d24a114c7fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae245df6f5c2459cb89028ed48ff63d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57ec5fed78f7467688526565e14d0559",
            "placeholder": "​",
            "style": "IPY_MODEL_2e882adc024b414aaa9fc39ace735653",
            "value": " 100/100 [00:00&lt;00:00, 468.64 examples/s]"
          }
        },
        "b5ffbc3f46f84cc3a7d773a4e66d6023": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e041f0dbc1b3487b8cdef04a8d6d390a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a8c05ebe25344baa7dbf4bbf31c3bfc",
              "IPY_MODEL_71eb642f30c6460bb05f4dfdb7e6fbb1",
              "IPY_MODEL_ae245df6f5c2459cb89028ed48ff63d0"
            ],
            "layout": "IPY_MODEL_36c89c53897e45628ee8b8596a131f71"
          }
        },
        "e823db771edb4f6e82fd72e0f081beec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
