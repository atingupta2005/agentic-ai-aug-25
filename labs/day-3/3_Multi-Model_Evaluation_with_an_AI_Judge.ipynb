{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Comparing and Evaluating Multiple LLMs\n",
    "\n",
    "This lab explores how to interact with various Large Language Models (LLMs) from different providers through their APIs. We will then implement an agentic pattern where one LLM evaluates the responses of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports ===\n",
    "# A good practice is to group all imports at the top of the script.\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI      # For OpenAI, Groq, DeepSeek, and Google (using compatible endpoint)\n",
    "from anthropic import Anthropic  # For Claude models\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from the .env file.\n",
    "# This is a crucial step to securely manage your API keys.\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === API Key Verification ===\n",
    "# This cell verifies that all necessary API keys are available in the environment.\n",
    "# It's a useful debugging step to ensure your setup is correct.\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "def check_key(name, key):\n",
    "    if key:\n",
    "        # Print only a small, non-sensitive prefix of the key.\n",
    "        print(f\"{name} Key is set (starts with: {key[:4]}...)\")\n",
    "    else:\n",
    "        # Some keys are optional for this lab.\n",
    "        print(f\"{name} Key is not set (optional)\")\n",
    "\n",
    "check_key(\"OpenAI\", openai_api_key)\n",
    "check_key(\"Anthropic\", anthropic_api_key)\n",
    "check_key(\"Google\", google_api_key)\n",
    "check_key(\"DeepSeek\", deepseek_api_key)\n",
    "check_key(\"Groq\", groq_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generate a Challenge Question\n",
    "\n",
    "First, we'll use an LLM to generate a single, high-quality question that we can then pose to all the other models. This ensures a fair and consistent comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation or preamble.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\", # Using a powerful model to generate a high-quality question\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(\"The generated question for all models is:\")\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Query Each Model with the Same Question\n",
    "\n",
    "Now we'll iterate through a list of different models from various providers. We will store each model's name and its answer for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store the names of the models and their answers.\n",
    "competitors = []\n",
    "answers = []\n",
    "\n",
    "# The message list now contains the single question we generated.\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Competitor 1: OpenAI (GPT-4o Mini) ===\n",
    "model_name = \"gpt-4o-mini\"\n",
    "print(f\"--- Querying {model_name} ---\")\n",
    "\n",
    "response = openai_client.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Competitor 2: Anthropic (Claude Sonnet) ===\n",
    "# Note: Anthropic uses a slightly different client and response structure.\n",
    "if anthropic_api_key:\n",
    "    model_name = \"claude-3-5-sonnet-20240620\"\n",
    "    print(f\"--- Querying {model_name} ---\")\n",
    "\n",
    "    claude_client = Anthropic()\n",
    "    # `max_tokens` is a required parameter for Anthropic's API.\n",
    "    response = claude_client.messages.create(model=model_name, messages=messages, max_tokens=2048)\n",
    "    # The answer is located in `response.content[0].text`.\n",
    "    answer = response.content[0].text\n",
    "\n",
    "    display(Markdown(answer))\n",
    "    competitors.append(model_name)\n",
    "    answers.append(answer)\n",
    "else:\n",
    "    print(\"Skipping Anthropic model, no API key found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Competitor 3: Google (Gemini Flash) ===\n",
    "# We can use the OpenAI client by pointing the `base_url` to Google's OpenAI-compatible endpoint.\n",
    "if google_api_key:\n",
    "    model_name = \"gemini-1.5-flash-latest\"\n",
    "    print(f\"--- Querying {model_name} ---\")\n",
    "    \n",
    "    # Note the custom base_url to route requests to Google.\n",
    "    gemini_client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/models\")\n",
    "    \n",
    "    # We need to specify the model as part of the endpoint in this case\n",
    "    response = gemini_client.chat.completions.create(model=f\"{model_name}:generateContent\", messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    display(Markdown(answer))\n",
    "    competitors.append(model_name)\n",
    "    answers.append(answer)\n",
    "else:\n",
    "    print(\"Skipping Google model, no API key found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Competitor 4: DeepSeek ===\n",
    "# DeepSeek also provides an OpenAI-compatible endpoint.\n",
    "if deepseek_api_key:\n",
    "    model_name = \"deepseek-chat\"\n",
    "    print(f\"--- Querying {model_name} ---\")\n",
    "\n",
    "    deepseek_client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "    response = deepseek_client.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    display(Markdown(answer))\n",
    "    competitors.append(model_name)\n",
    "    answers.append(answer)\n",
    "else:\n",
    "    print(\"Skipping DeepSeek model, no API key found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Competitor 5: Groq (Llama 3) ===\n",
    "# Groq offers very fast inference on models like Llama via their compatible endpoint.\n",
    "if groq_api_key:\n",
    "    model_name = \"llama3-70b-8192\"\n",
    "    print(f\"--- Querying {model_name} ---\")\n",
    "\n",
    "    groq_client = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "    response = groq_client.chat.completions.create(model=model_name, messages=messages)\n",
    "    answer = response.choices[0].message.content\n",
    "\n",
    "    display(Markdown(answer))\n",
    "    competitors.append(model_name)\n",
    "    answers.append(answer)\n",
    "else:\n",
    "    print(\"Skipping Groq model, no API key found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ollama for Local Models\n",
    "\n",
    "Ollama allows you to run open-source models directly on your own machine. It exposes a local server that is compatible with the OpenAI API, making it easy to integrate.\n",
    "\n",
    "1.  **Installation**: If you haven't already, [download Ollama](https://ollama.com).\n",
    "2.  **Run the Server**: Open a terminal and run `ollama serve`. You should see \"Ollama is running\" at [http://localhost:11434](http://localhost:11434).\n",
    "3.  **Pull a Model**: In your terminal, pull a model to use. We'll use `llama3`, a powerful and reasonably sized model.\n",
    "    ```bash\n",
    "    ollama pull llama3\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Competitor 6: Ollama (Local Llama 3) ===\n",
    "# To use Ollama, we point the OpenAI client to the local server address.\n",
    "# An API key is required but its value doesn't matter for local Ollama.\n",
    "\n",
    "try:\n",
    "    model_name = \"llama3\"\n",
    "    print(f\"--- Querying {model_name} (local) ---\")\n",
    "    ollama_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "    # We can check if the model exists locally before calling it.\n",
    "    local_models = [m['name'] for m in ollama_client.models.list().data]\n",
    "    if f\"{model_name}:latest\" in local_models:\n",
    "        response = ollama_client.chat.completions.create(model=model_name, messages=messages)\n",
    "        answer = response.choices[0].message.content\n",
    "\n",
    "        display(Markdown(answer))\n",
    "        competitors.append(model_name)\n",
    "        answers.append(answer)\n",
    "    else:\n",
    "        print(f\"Ollama model '{model_name}' not found. Please run 'ollama pull {model_name}' in your terminal.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to Ollama. Is the server running? Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the Results with a Judge LLM\n",
    "\n",
    "Now that we have all the responses, we can implement an **Evaluator** pattern. We will assemble all the answers into a single prompt and ask a powerful LLM to act as a judge, ranking the responses from best to worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's combine all the answers into a single string for the judge's prompt.\n",
    "# The `enumerate` function is used to get both the index and the value.\n",
    "\n",
    "all_answers_text = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    all_answers_text += f\"# Response from competitor {index+1} ({competitors[index]})\\n\\n\"\n",
    "    all_answers_text += answer + \"\\n\\n---\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the master prompt for our Judge agent.\n",
    "# It includes the original question, all the answers, and instructions for the output format (JSON).\n",
    "# Using f-strings makes it easy to inject our variables into the prompt text.\n",
    "\n",
    "judge_prompt = f\"\"\"You are an impartial judge in a competition between {len(competitors)} Large Language Models.\n",
    "Your task is to evaluate each model's response for clarity, depth, accuracy, and strength of argument.\n",
    "\n",
    "The models were all asked this question:\n",
    "--- QUESTION ---\n",
    "{question}\n",
    "--- END QUESTION ---\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "--- COMPETITOR RESPONSES ---\n",
    "{all_answers_text}\n",
    "--- END RESPONSES ---\n",
    "\n",
    "Please rank the competitors from best to worst based on their answers.\n",
    "Respond with JSON, and only JSON. The JSON object should have a single key, \"results\", which is a list of the competitor numbers (as integers) in ranked order.\n",
    "Example format: {{\"results\": [3, 1, 2, 4]}}\n",
    "Do not include any other text, explanations, or markdown formatting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the final prompt before sending it.\n",
    "print(judge_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time for the final judgement!\n",
    "# We will use GPT-4o for this task as it has strong reasoning and instruction-following capabilities.\n",
    "# We also enable JSON mode to ensure the output is valid JSON.\n",
    "\n",
    "judge_messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n",
    "\n",
    "judgement_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=judge_messages,\n",
    "    response_format={\"type\": \"json_object\"} # Enforce JSON output\n",
    ")\n",
    "results_json = judgement_response.choices[0].message.content\n",
    "print(\"Raw JSON from judge:\")\n",
    "print(results_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final Results ===\n",
    "# Now we parse the JSON response from the judge and print the final rankings.\n",
    "\n",
    "try:\n",
    "    results_dict = json.loads(results_json)\n",
    "    ranks = results_dict[\"results\"]\n",
    "    print(\"\\n--- FINAL RANKINGS ---\")\n",
    "    for rank_index, competitor_number in enumerate(ranks):\n",
    "        # The competitor number is 1-based, so we subtract 1 for the list index.\n",
    "        competitor_name = competitors[int(competitor_number)-1]\n",
    "        print(f\"Rank {rank_index+1}: {competitor_name}\")\n",
    "except (json.JSONDecodeError, KeyError, IndexError) as e:\n",
    "    print(f\"\\nError parsing the judge's response. Please check the raw JSON output. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
