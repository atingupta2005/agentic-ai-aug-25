{"cells":[{"cell_type":"markdown","metadata":{"id":"hNUW4ga5P23Z"},"source":["# Lab: Comparing and Evaluating Multiple LLMs\n","\n","This lab explores how to interact with various Large Language Models (LLMs) from different providers through their APIs. We will then implement an agentic pattern where one LLM evaluates the responses of others."]},{"cell_type":"code","source":["!pip install dotenv anthropic"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOr-K7ucQMJP","executionInfo":{"status":"ok","timestamp":1754313912341,"user_tz":-330,"elapsed":8705,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}},"outputId":"3b311c72-cae6-4f35-fc0f-be9f75730967"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: dotenv in /usr/local/lib/python3.11/dist-packages (0.9.9)\n","Collecting anthropic\n","  Downloading anthropic-0.60.0-py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from dotenv) (1.1.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.14.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.7.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n","Downloading anthropic-0.60.0-py3-none-any.whl (293 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.1/293.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: anthropic\n","Successfully installed anthropic-0.60.0\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"a_M-59mpP23e","executionInfo":{"status":"ok","timestamp":1754313912364,"user_tz":-330,"elapsed":6,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"MmlpviQ1P23g","executionInfo":{"status":"ok","timestamp":1754313912715,"user_tz":-330,"elapsed":348,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}}},"outputs":[],"source":["# === Imports ===\n","# A good practice is to group all imports at the top of the script.\n","\n","import os\n","import json\n","from dotenv import load_dotenv\n","from openai import OpenAI      # For OpenAI, Groq, DeepSeek, and Google (using compatible endpoint)\n","from anthropic import Anthropic  # For Claude models\n","from IPython.display import Markdown, display"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vD3yG6GP23h","executionInfo":{"status":"ok","timestamp":1754313916389,"user_tz":-330,"elapsed":28,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}},"outputId":"c0337648-f338-46f5-b3d5-bd941484a62e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}],"source":["# Load environment variables from the .env file.\n","# This is a crucial step to securely manage your API keys.\n","load_dotenv(\"dev.env\", override=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3J_Uqa5qP23i","executionInfo":{"status":"ok","timestamp":1754313919406,"user_tz":-330,"elapsed":21,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}},"outputId":"bf26de99-8cea-4f87-87fc-76546f7720c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["OpenAI Key is set (starts with: sk-p...)\n","Anthropic Key is not set (optional)\n","Google Key is not set (optional)\n","DeepSeek Key is not set (optional)\n","Groq Key is not set (optional)\n"]}],"source":["# === API Key Verification ===\n","# This cell verifies that all necessary API keys are available in the environment.\n","# It's a useful debugging step to ensure your setup is correct.\n","\n","openai_api_key = os.getenv('OPENAI_API_KEY')\n","anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n","google_api_key = os.getenv('GOOGLE_API_KEY')\n","deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n","groq_api_key = os.getenv('GROQ_API_KEY')\n","\n","def check_key(name, key):\n","    if key:\n","        # Print only a small, non-sensitive prefix of the key.\n","        print(f\"{name} Key is set (starts with: {key[:4]}...)\")\n","    else:\n","        # Some keys are optional for this lab.\n","        print(f\"{name} Key is not set (optional)\")\n","\n","check_key(\"OpenAI\", openai_api_key)\n","check_key(\"Anthropic\", anthropic_api_key)\n","check_key(\"Google\", google_api_key)\n","check_key(\"DeepSeek\", deepseek_api_key)\n","check_key(\"Groq\", groq_api_key)"]},{"cell_type":"markdown","metadata":{"id":"fgZturwiP23i"},"source":["### Step 1: Generate a Challenge Question\n","\n","First, we'll use an LLM to generate a single, high-quality question that we can then pose to all the other models. This ensures a fair and consistent comparison."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"TESkRf8mP23i","executionInfo":{"status":"ok","timestamp":1754313928569,"user_tz":-330,"elapsed":4,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}}},"outputs":[],"source":["request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n","request += \"Answer only with the question, no explanation or preamble.\"\n","messages = [{\"role\": \"user\", \"content\": request}]"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zgdYMnUEP23j","executionInfo":{"status":"ok","timestamp":1754313933054,"user_tz":-330,"elapsed":3148,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}},"outputId":"b7fd685f-c5c2-44c2-9ee6-c5fefa444880"},"outputs":[{"output_type":"stream","name":"stdout","text":["The generated question for all models is:\n","How might the implementation of universal basic income impact the balance of socio-economic power and individual motivation in societies with differing cultural attitudes toward work and welfare?\n"]}],"source":["openai_client = OpenAI()\n","response = openai_client.chat.completions.create(\n","    model=\"gpt-4o\", # Using a powerful model to generate a high-quality question\n","    messages=messages,\n",")\n","question = response.choices[0].message.content\n","print(\"The generated question for all models is:\")\n","print(question)"]},{"cell_type":"markdown","metadata":{"id":"_cHQvPhGP23j"},"source":["### Step 2: Query Each Model with the Same Question\n","\n","Now we'll iterate through a list of different models from various providers. We will store each model's name and its answer for later evaluation."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"S0Aw6XN4P23j","executionInfo":{"status":"ok","timestamp":1754313937078,"user_tz":-330,"elapsed":12,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}}},"outputs":[],"source":["# Initialize lists to store the names of the models and their answers.\n","competitors = []\n","answers = []\n","\n","# The message list now contains the single question we generated.\n","messages = [{\"role\": \"user\", \"content\": question}]"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":826},"id":"3N8eEguNP23k","executionInfo":{"status":"ok","timestamp":1754313954639,"user_tz":-330,"elapsed":13432,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}},"outputId":"d89402ec-7c41-4c14-e981-8300cf8dd5e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Querying gpt-4o-mini ---\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"The implementation of Universal Basic Income (UBI) can have profound effects on socio-economic power dynamics and individual motivation, especially in societies with varying cultural attitudes toward work and welfare. Here are several key considerations:\n\n### 1. **Socio-Economic Power Dynamics**\n\n- **Redistribution of Wealth**: UBI can help reduce income inequality by providing a baseline financial support to all citizens, which may shift socio-economic power away from traditional wealth holders and create a more equitable distribution of resources. In cultures that value egalitarianism, this may empower previously marginalized groups.\n\n- **Influence on Labor Markets**: In societies where work is closely tied to social status, UBI might challenge the conventional norms around employment. Individuals may feel more empowered to seek jobs that are meaningful or aligned with their skills and interests rather than taking any job for survival. Conversely, in cultures where there is a strong work ethic, UBI could be viewed as undermining the motivation to work, potentially leading to social disapproval among those who value hard work.\n\n### 2. **Individual Motivation**\n\n- **Security and Creativity**: In cultures that promote innovation and creativity, UBI can provide individuals with the financial security needed to pursue entrepreneurial endeavors or artistic expressions without the immediate pressure of financial survival. This could lead to increased creativity and a richer cultural landscape.\n\n- **Work Ethic and Identity**: In societies where work is integral to personal identity and social standing, some individuals may perceive UBI as a threat to their sense of purpose. This could lead to resistance to UBI or calls for conditions. The motivation to engage in work might diminish for some, while others could find it liberating and motivating to engage in work that they genuinely value.\n\n- **Impact on Labor Participation**: The effect of UBI on labor market participation varies by culture. In cultures that prioritize individualism and self-improvement, UBI may encourage individuals to invest time in education or skills development, reducing immediate participation in traditional employment. In contrast, in collectivist cultures where community and duty are emphasized, UBI might serve as a support system to allow individuals to contribute more fully to their communities in non-traditional ways.\n\n### 3. **Cultural Attitudes Toward Welfare**\n\n- **Acceptance of Welfare Systems**: In regions where welfare systems are seen as a social safety net, UBI may be readily accepted and might enhance trust in governmental institutions. However, in cultures with a stigma around receiving government support, UBI could be perceived negatively, impacting its effectiveness and acceptance.\n\n- **Cross-Cultural Variability**: Cultural attitudes towards dependency and welfare can lead to varying receptions of UBI. In some cultures, there may be a strong belief in self-sufficiency, which could create skepticism about UBI. In these societies, messaging around UBI would need to emphasize empowerment and agency rather than dependency.\n\n### 4. **Long-term Social Change**\n\n- **Rethinking Work and Value**: Over time, UBI could encourage a reevaluation of what constitutes valuable work. Societies may increasingly recognize the value of caregiving, volunteering, and other forms of labor that are currently undervalued in traditional economic models.\n\n- **Potential for Social Cohesion or Division**: UBI might foster greater social cohesion by addressing poverty and economic insecurity, promoting a sense of shared responsibility. However, if not implemented carefully or coupled with ongoing public dialogue, it could also lead to divisions based on differing beliefs about work, productivity, and entitlement.\n\nIn summary, the impact of UBI on socio-economic power dynamics and individual motivation will depend significantly on the cultural context in which it is implemented. Understanding and addressing these cultural attitudes is essential for the successful implementation of UBI, ensuring it serves as a tool for empowerment rather than a source of division or discontent."},"metadata":{}}],"source":["# === Competitor 1: OpenAI (GPT-4o Mini) ===\n","model_name = \"gpt-4o-mini\"\n","print(f\"--- Querying {model_name} ---\")\n","\n","response = openai_client.chat.completions.create(model=model_name, messages=messages)\n","answer = response.choices[0].message.content\n","\n","display(Markdown(answer))\n","competitors.append(model_name)\n","answers.append(answer)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"exK8JJntP23k","executionInfo":{"status":"ok","timestamp":1754313977694,"user_tz":-330,"elapsed":28,"user":{"displayName":"Atin Gupta","userId":"01235820350389977490"}},"outputId":"53b85dc5-815c-4852-9ddb-5dc58049df4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Skipping Anthropic model, no API key found.\n"]}],"source":["# === Competitor 2: Anthropic (Claude Sonnet) ===\n","# Note: Anthropic uses a slightly different client and response structure.\n","if anthropic_api_key:\n","    model_name = \"claude-3-5-sonnet-20240620\"\n","    print(f\"--- Querying {model_name} ---\")\n","\n","    claude_client = Anthropic()\n","    # `max_tokens` is a required parameter for Anthropic's API.\n","    response = claude_client.messages.create(model=model_name, messages=messages, max_tokens=2048)\n","    # The answer is located in `response.content[0].text`.\n","    answer = response.content[0].text\n","\n","    display(Markdown(answer))\n","    competitors.append(model_name)\n","    answers.append(answer)\n","else:\n","    print(\"Skipping Anthropic model, no API key found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_l1dG0WP23l"},"outputs":[],"source":["# === Competitor 3: Google (Gemini Flash) ===\n","# We can use the OpenAI client by pointing the `base_url` to Google's OpenAI-compatible endpoint.\n","if google_api_key:\n","    model_name = \"gemini-1.5-flash-latest\"\n","    print(f\"--- Querying {model_name} ---\")\n","\n","    # Note the custom base_url to route requests to Google.\n","    gemini_client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/models\")\n","\n","    # We need to specify the model as part of the endpoint in this case\n","    response = gemini_client.chat.completions.create(model=f\"{model_name}:generateContent\", messages=messages)\n","    answer = response.choices[0].message.content\n","\n","    display(Markdown(answer))\n","    competitors.append(model_name)\n","    answers.append(answer)\n","else:\n","    print(\"Skipping Google model, no API key found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EqfBV2_GP23l"},"outputs":[],"source":["# === Competitor 4: DeepSeek ===\n","# DeepSeek also provides an OpenAI-compatible endpoint.\n","if deepseek_api_key:\n","    model_name = \"deepseek-chat\"\n","    print(f\"--- Querying {model_name} ---\")\n","\n","    deepseek_client = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n","    response = deepseek_client.chat.completions.create(model=model_name, messages=messages)\n","    answer = response.choices[0].message.content\n","\n","    display(Markdown(answer))\n","    competitors.append(model_name)\n","    answers.append(answer)\n","else:\n","    print(\"Skipping DeepSeek model, no API key found.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqCo99yBP23l"},"outputs":[],"source":["# === Competitor 5: Groq (Llama 3) ===\n","# Groq offers very fast inference on models like Llama via their compatible endpoint.\n","if groq_api_key:\n","    model_name = \"llama3-70b-8192\"\n","    print(f\"--- Querying {model_name} ---\")\n","\n","    groq_client = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n","    response = groq_client.chat.completions.create(model=model_name, messages=messages)\n","    answer = response.choices[0].message.content\n","\n","    display(Markdown(answer))\n","    competitors.append(model_name)\n","    answers.append(answer)\n","else:\n","    print(\"Skipping Groq model, no API key found.\")"]},{"cell_type":"markdown","metadata":{"id":"GiGnQ7D_P23m"},"source":["### Using Ollama for Local Models\n","\n","Ollama allows you to run open-source models directly on your own machine. It exposes a local server that is compatible with the OpenAI API, making it easy to integrate.\n","\n","1.  **Installation**: If you haven't already, [download Ollama](https://ollama.com).\n","2.  **Run the Server**: Open a terminal and run `ollama serve`. You should see \"Ollama is running\" at [http://localhost:11434](http://localhost:11434).\n","3.  **Pull a Model**: In your terminal, pull a model to use. We'll use `llama3`, a powerful and reasonably sized model.\n","    ```bash\n","    ollama pull llama3\n","    ```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCC4YkBrP23m"},"outputs":[],"source":["# === Competitor 6: Ollama (Local Llama 3) ===\n","# To use Ollama, we point the OpenAI client to the local server address.\n","# An API key is required but its value doesn't matter for local Ollama.\n","\n","try:\n","    model_name = \"llama3\"\n","    print(f\"--- Querying {model_name} (local) ---\")\n","    ollama_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n","\n","    # We can check if the model exists locally before calling it.\n","    local_models = [m['name'] for m in ollama_client.models.list().data]\n","    if f\"{model_name}:latest\" in local_models:\n","        response = ollama_client.chat.completions.create(model=model_name, messages=messages)\n","        answer = response.choices[0].message.content\n","\n","        display(Markdown(answer))\n","        competitors.append(model_name)\n","        answers.append(answer)\n","    else:\n","        print(f\"Ollama model '{model_name}' not found. Please run 'ollama pull {model_name}' in your terminal.\")\n","\n","except Exception as e:\n","    print(f\"Could not connect to Ollama. Is the server running? Error: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"5pYh7bjaP23m"},"source":["### Step 3: Evaluate the Results with a Judge LLM\n","\n","Now that we have all the responses, we can implement an **Evaluator** pattern. We will assemble all the answers into a single prompt and ask a powerful LLM to act as a judge, ranking the responses from best to worst."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDQPSjTYP23n"},"outputs":[],"source":["# First, let's combine all the answers into a single string for the judge's prompt.\n","# The `enumerate` function is used to get both the index and the value.\n","\n","all_answers_text = \"\"\n","for index, answer in enumerate(answers):\n","    all_answers_text += f\"# Response from competitor {index+1} ({competitors[index]})\\n\\n\"\n","    all_answers_text += answer + \"\\n\\n---\\n\\n\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"usAQOL2wP23n"},"outputs":[],"source":["# This is the master prompt for our Judge agent.\n","# It includes the original question, all the answers, and instructions for the output format (JSON).\n","# Using f-strings makes it easy to inject our variables into the prompt text.\n","\n","judge_prompt = f\"\"\"You are an impartial judge in a competition between {len(competitors)} Large Language Models.\n","Your task is to evaluate each model's response for clarity, depth, accuracy, and strength of argument.\n","\n","The models were all asked this question:\n","--- QUESTION ---\n","{question}\n","--- END QUESTION ---\n","\n","Here are the responses from each competitor:\n","--- COMPETITOR RESPONSES ---\n","{all_answers_text}\n","--- END RESPONSES ---\n","\n","Please rank the competitors from best to worst based on their answers.\n","Respond with JSON, and only JSON. The JSON object should have a single key, \"results\", which is a list of the competitor numbers (as integers) in ranked order.\n","Example format: {{\"results\": [3, 1, 2, 4]}}\n","Do not include any other text, explanations, or markdown formatting.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kFkyX_FHP23n"},"outputs":[],"source":["# Let's see the final prompt before sending it.\n","print(judge_prompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrZ_kkfEP23n"},"outputs":[],"source":["# It's time for the final judgement!\n","# We will use GPT-4o for this task as it has strong reasoning and instruction-following capabilities.\n","# We also enable JSON mode to ensure the output is valid JSON.\n","\n","judge_messages = [{\"role\": \"user\", \"content\": judge_prompt}]\n","\n","judgement_response = openai_client.chat.completions.create(\n","    model=\"gpt-4o\",\n","    messages=judge_messages,\n","    response_format={\"type\": \"json_object\"} # Enforce JSON output\n",")\n","results_json = judgement_response.choices[0].message.content\n","print(\"Raw JSON from judge:\")\n","print(results_json)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sG6fuMhlP23n"},"outputs":[],"source":["# === Final Results ===\n","# Now we parse the JSON response from the judge and print the final rankings.\n","\n","try:\n","    results_dict = json.loads(results_json)\n","    ranks = results_dict[\"results\"]\n","    print(\"\\n--- FINAL RANKINGS ---\")\n","    for rank_index, competitor_number in enumerate(ranks):\n","        # The competitor number is 1-based, so we subtract 1 for the list index.\n","        competitor_name = competitors[int(competitor_number)-1]\n","        print(f\"Rank {rank_index+1}: {competitor_name}\")\n","except (json.JSONDecodeError, KeyError, IndexError) as e:\n","    print(f\"\\nError parsing the judge's response. Please check the raw JSON output. Error: {e}\")"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}